% vim: tw=80 spell spelllang=en

\fontfam[lm]
\input ctustyle3
%\load[vlna]
%\singlechars{Czech}{AaIiVvOoUuSsZzKk}
\enlang
\enquotes
\verbchar`
\picdir={figures/}
\nonstopmode
\nonumcitations

\draft

\newcount \_secccnum
\def \_thesecccnum {\_othe\_chapnum.\_the\_secnum.\_the\_seccnum.\_the\_secccnum}

\_optdef\_seccc[]{\_trylabel \_scantoeol\_inseccc}

\_def\_inseccc #1{\_par \_sectionlevel=4
   \_def \_savedtitle {#1}% saved to .ref file
   \_ifnonum \_else {\_globaldefs=1 \_incr\_secccnum \_secccx}\_fi
   \_edef \_therefnum {\_ifnonum \_space \_else \_thesecccnum \_fi}%
   \_printseccc{\_scantextokens{#1}}%
   \_resetnonumnotoc
}
\public \seccc ;

\_def \_chapx {\_secx   \_secnum=0   \_lfnotenum=0 }
\_def \_secx  {\_seccx  \_seccnum=0  \_tnum=0 \_fnum=0 \_dnum=0 \_resetABCDE }
\_def \_seccx {\_secccx \_secccnum=0 }
\_def \_secccx {}


\_def\_printseccc#1{\_par
   \_abovetitle{\_penalty-100}{\_medskip}
   {\_bf \_noindent \_raggedright \_printrefnum[@\_quad]#1\_nbpar}%
   \_nobreak \_belowtitle{\_smallskip}%
   \_firstnoindent
}

\_def\secccfont{\_scalemain\_typoscale[\_magstephalf/\_magstephalf]\_ctustyle_boldify}
\_def\_printseccc#1{\_par \_abovetitle{\_goodbreak} 
  \_smallskip\_medskip\_vskip.5\_parskip
  \_ctustyle_begitemstest\seccc
  \_line{%\Blue\_vrule height 3.5mm width4mm depth.3mm\Black 
  \_hss\_vtop{\_advance\_hsize by-0mm
     \secccfont \_noindent \_printrefnum[@\_quad]%
     \_ctustyle_nBlue#1\_rightskip=0pt plus1fil \_strut\_nbpar\_kern-4.5pt}}%
  \_nobreak\_vskip-.5\_parskip\_smallskip\_vskip2pt\_relax
  \_firstnoindent
}


%\_sdef{_tocl:4}#1#2#3{\_advance\_leftskip by2\_iindent \_cs{_tocl:2}{#1}{#2}{#3}}

\newcount\tnotenum
\def\tnotelist{}
\def\tnote#1{\incr\tnotenum $^{\rm\_romannumeral\tnotenum}$\global\addto\tnotelist{{#1}}}
\def\tnoteprint{\par \tnotenum=0
   \ea\foreach\tnotelist
     \do{\advance\tnotenum by1 \par $^{\rm\_romannumeral\tnotenum}$##1 }\par
   \global\tnotenum=0 \gdef\tnotelist{}%
}

\def\pdfstdcite[#1]{[\rcite[iso32000-1], část~#1]}

\newdimen\halfhsize \halfhsize=\dimexpr\hsize/2-2pt\relax

\hyphenation{hon-or mark-up}

\_def\_urlskip{\_null\_nobreak\_hskip0pt plus0.1em\_relax}
\_def\_urlbskip{\_penalty50 \_hskip0pt plus0.1em\_relax}

\bibtexhook={
\_sdef{_print:misc}{%
   \_bprintb [!author]    {\_doauthor1{##1}\.\ }{\_bibwarning}%
   \_bprintb [title]      {{\_em##1}\_bprintc\_titlepost{\.\ *}\_bprintv[howpublished]{}{\.}\ }%
                                                                                     {\_bibwarning}%
   \_bprinta [howpublished]  {[*].\ }{}%
  %\_bprinta [ednote]     {\_prepareednote*\_bprintv[citedate]{}{.}\ }{\_bibwarning}%
   \_bprinta [ednote]     {\_prepareednote*\_bprintv[citedate]{}{.}\ }{}%
   \_bprintb [year]       {\_doyear{##1}\_bprintv[citedate]{}{.}\ }{\_bibwarninga}%
  %\_bprintb [year]       {\_doyear{##1}\_bprintv[citedate]{.}{.}\ }{\_bibwarninga}%
   \_bprinta [citedate]   {\_docitedate*///\_relax.\ }{}%
   \_bprintb [doi]        {\_predoi DOI \_ulink[http://dx.doi.org/##1]{##1}.\ }{}%
   \_bprintb [url]        {\_preurl\_url{##1}. }{}%
}
}

\def\optparams{\adef<##1>{\hbox{$\langle$\it##1\/$\rangle$}}}
\toksapp\everyintt{\optparams}
\toksapp\everytt{\typosize[9.5/12.4]}

\worktype [M/EN]

\faculty{F8}

\department{Department of Theoretical Computer Science}
\title  {x86-64 native backend for TinyC}
\titleCZ{x86-64 nativní backend pro TinyC}
\author{Michal Vlasák}
\date{XX.\,XX.\,TODO}
\supervisor{Ing. Petr Máj}
\abstractEN {
}

\abstractCZ {
}

\keywordsEN {%
TODO
}
\keywordsCZ {%
TODO
}
\thanks {% Use main language here
}

\declaration {
I hereby declare that the presented thesis is my own work and that I have cited
all sources of information in accordance with the Guideline for adhering to
ethical principles when elaborating an academic final thesis.

I acknowledge that my thesis is subject to the rights and obligations stipulated
by the Act No. 121/2000 Coll., the Copyright Act, as amended. In accordance with
Section 2373(2) of Act No. 89/2012 Coll., the Civil Code, as amended, I hereby
grant a non-exclusive authorization (licence) to utilize this thesis, including
all computer programs that are part of it or attached to it and all
documentation thereof (hereinafter collectively referred to as the "Work"), to
any and all persons who wish to use the Work. Such persons are entitled to use
the Work in any manner that does not diminish the value of the Work and for any
purpose (including use for profit). This authorisation is unlimited in time,
territory and quantity.

V XX dne XX.\,XX.\,TODO
\signature
}

{\nopagenumbers
  {\pgbackground={
    \picwidth=\pagewidth \picheight=\pageheight
    \inspic{vlasami6-assignment.pdf}}
    \null\vfil\break}
  \null\vfil\break}
\makefront

\chap Introduction



\chap x86-64 architecture

AKA AMD64

dlouhá historie

původně 16 bit,


\sec Characteristics

CISC

Two-address code

kódování založené na osmičkách (3 bity) => 8 registrů, 8 binárních operací, 8
unárních operací, 8 shiftů, apod.

podporuje 8, 16, 32 a 64-bit operace, z pohledu efektivity (partial register
stalls) a jazyka C se vyplatí používat jen 32 a 64 bitové operace (zbytek movzx,
movsx nebo čtení z užších podregistrů)

omezení často vychází z kódování instrukcí (shifty a dělení jsou kódovány jako unární operace
a nemají v kódování místo na víc než jeden argument)

\secc Addressing modes

ModRM bajt, SIB adresování + RIP relativní adresování

Max. jeden paměťový operand

Dvouadresový kód - kódování stále stejné, jen směr otáčí jeden bit v opcodu

\sec Calling conventions

System V AMD64 ABI. 

Registry pro argumenty, návratové hodnoty, caller saved, callee saved

\sec Comparison with t86

Srovnání s t86

\chap State of the art

kapitola o představení teorie za různými částmi překladače a zmínka zavedených
existujících algoritmů a jejich přístupů

\sec Structure of a compiler backend

Instruction selection + Instruction scheduling + Register allocation

Je potřeba zvážit middle end, jedná se o vstup

Middle end často SSA, je potřeba s tím počítat (SSA dekonstrukce) nebo to rovnou
využít (register allocation na SSA)

\secc Phase ordering

Backend fáze na sobě vzájemně závisí

regalloc vytváří instrukce

selection neví co nakonec bude v registrech

selection neví jaké registry nakonec budou použity

Klasikcé řešení selection -> scheduling -> regalloc je přijatelný kompromis

\sec SSA form

SSA ({\em static single assignment}) form is a form used by most of today's
optimizing compilers, including for example LLVM and GCC. It simplifies and
makes faster a lot of classic optimizations.

Like the name suggests, static single assignment form stands on the fact
that each variable is assigned exactly once. On top of that, we are interested
only in {\em static} assignments, that is, there is only one program point that
assigns the variable (contrary to {\em dynamic} assignments, which would count
how many times the assignment is executed at runtime, i.e.\ how many times the
execution gets to the single program point which assigns the variable).

There are many important benefits to SSA form, of which we highlight a few:

\begitems
* Since each variable is assigned only once, no variable is ever reassigned.
Thus value held by the variable is always available. This means that algorithms
don't have to be cautious about using a definition that is reassigned.

* There is no ambiguity in what {\em definition} of a variable a {\em use} can
refer to, since each variable has exactly one definition. This means that
instead of using {\em use-def chains} (a data-structure that links together all
definitions that may reach a use), uses can refer directly to the unique
definition.
\enditems

As an example, here is an example that is not in SSA form, because `a` is
assigned twice:

\begtt \adef![#1]{\url{#1}}
a = 1
b = 1;
a = 2;
return (a + b) * (a + b);
\endtt

A human can easily tell, that the first store to `a` is dead, since `a` is
reassigned. It is also easy to tell that the both of the expressions `a * b`
compute the same result, since they refer to the same `a` and `b`. However, this
is not as easy to tell for a compiler. But \"versioning" each definition of a
variable and tracking which version gets used makes the observation simpler for
an optimizing compiler:

\begtt \adef!#1{$_{\tt #1}$}
a!1 = 1
b!1 = 1;
a!2 = 2;
return (a!2 + b!1) * (a!2 + b!1);
\endtt

\def\v#1#2{\code{#1}$_{\tt #2}$}

Here definition \v{a}{1} has no uses, this can be seen by keeping {\em def-use
chains} (which link together all uses of a definition, and which are not to be
confused with {\em use-def chains} mentioned above). But, the fact that \v a2 is
defined, doesn't make \v a1 unavailable and if it held more interesting value
then a constant, an optimizing compiler could use it even after the definition
of \v a2. It is also much easier to tell that the `a + b` expressions are indeed
the same, since now they are the result of applying the same operator to the
exact same versions of variables, regardless of how many definitions these
variables had. The implementation of SSA construction by versioning the
definitions is also very easy to implement.

Constructing SSA becomes more problematic with conditional execution:

\begtt \adef!#1{$_{\tt #1}$}
b = 1;
if (a) {
    b = -b;
}
return b;
\endtt

When we try to version the variables here, the we don't know whether to continue
with \v b 1 or \v b 2 after the `if` statement, though we are able to tell just
fine that the use of `b` in the conditional branch corresponds to \v b 1:

\begtt \adef!#1{$_{\tt #1}$}
b!1 = 1;
if (a!1) {
    b!2 = -b!1;
}
return b!?;
\endtt

The problem is that multiple definitions reach a use. With use-def chains this
could have been easily represented. But with SSA we don't want to use def-use
chains, since we want only one definition to reach each use. This brings us to
the idea of actually introducing a definition after the `if` statement, which
merges \v b 1 and \v b 2 into \v b 3, and can then be unambiguously used by
the return statement:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
b!1 = 1;
if (a!1) {
    b!2 = -b!1;
}
b!3 = |(b!1, b!2);
return b!3;
\endtt

Of course, the ambiguity is still there, just hidden behind the mysterious
$\phi$ (\"phi") function which provides the merging definition. We would
like the $\phi$ to evaluate to right version depending on the control flow which
occurs at run-time, so we define the $\phi$ function to do exactly that. Though
the semantics may seem a bit weird. Having the ambiguity in phi is is still
better than use-def chains, since there can be other uses of \v b 3 all
referring to the single $\phi$, instead of each having multiple reaching
definitions.

The simplest possible method for SSA construction inserts a $\phi$ instruction
for every variable at each merge point. This is correct, but such approach
introduces many redundant $\phi$s, that don't do nothing much useful. For
example in the example above a $\phi$ instruction would be introduced after the
conditional branch for `a`, even though there is a single definition:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
b!1 = 1;
if (a!1) {
    b!2 = -b!1;
}
a!2 = |(a!1, a!1);
b!3 = |(b!1, b!2);
return b!3;
\endtt

Similarly redundant $\phi$ functions are created even in loops:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
a = 1;
loop:
if (f()) goto loop;
\endtt

Here, even though there is not reference to `a` in the loop, a $\phi$ function
needed to be introduced, since execution may flow to the beginning of the loop
from two places (the code preceding it, or the end of the loop:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
a!1 = 1;
loop:
a!2 = |(a!1, a!2);
if (f()) goto loop;
a!3 = |(a!2, a!2);
\endtt

The $\phi$ refers to itself and only one other value (\v a 1), so it also can be
safely removed and the value used instead. Similar \"cyclic" $\phi$s can occur
even indirectly---two $\phi$ referring to each other and a single value, which
could be used directly.

So even though even simple SSA construction is possible, the produced code isn't
as useful because of many redundant $\phi$ nodes. The original inventors of SSA
form and the $\phi$ function concept~\cite[Rosen1988] came up with an efficient
algorithm for SSA construction~\cite[Cytron1991], which is based on the new
concept of {\em dominance frontiers}, which simply put are exactly the blocks
where $\phi$ functions are needed. Their algorithm produces what is known as
{\em minimal} SSA form, which doesn't contain redundant $\phi$s. Despite the
name, even more \"minimal" forms of SSA exist, for example {\em pruned} SSA
doesn't contain {\em dead} $\phi$ functions.

An important observation with regards to SSA form is, that since each variable
is assigned exactly once, and the variables don't get reassigned, there is no
need for the concept of a {\em variable}. The {\em values} assigned to the
variables can be used directly instead. Values are described by their structure,
for example number literals, like `5` or operations applied to other values
like `add 1, 2` (addition of two numbers) or `add 5, (sub 6, 7)` (addition of a
number and the result of subtraction of of two numbers). Generally we can divide
the values used in programs into two categories:

\begitems\style n
* {\em Constants}. These represent number or character literals, but also (addresses
of) static objects. Examples include `3` (integer literal), `'a'` (character
literal), `f` (address of a function), `g` (address of a global variable).

* {\em Operations}. These represent values produced from other values by
applying some arithmetic or other operation. Examples include `neg 5` is a
(unary) negation operation applied to a constant, `add 5, neg neg 3` is a binary
operation applied to a constant and a negation applied to negation of a constant
number 3. Arity isn't limited, and for example function calls can be seen as
operations on several values of which the first is the called function and the
rest are the arguments, for example we can have function `f` called with 4
arguments: `call f, 1, 2, 3, 4`.
\enditems

Even for example function parameters are constants---even though arguments
(actual parameters) passed to a function may just be results of some operations,
from the point of view of the function the formal parameters are constant values
that \"magically" materialize when the function begins its execution.

When represented in a compiler, the values are objects and they refer to each
other by means of references (for example through pointers or by means of
reference types). Our textual notation for operations so far was direct: a use
of value meant writing out the textual representation of the definition
(constructor) of a value. Because each value can be used many times and due to
the recursive nature (operations are able to refer to other operations), this
quickly becomes unwieldy. We will use a different notation that assigns each
value a number, $i$, and writes the references as `v`$_{\tt i}$. The definitions
are accompanied by a \"virtual assignment" whose purpose is to show what index
is used to represent references to the particular value. For example, we could
have:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
v!1 = 3
v!2 = 3
v!3 = mul v!1, v!2
\endtt

Which differs a bit from:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
v!1 = 3
v!3 = mul v!1, v!1
\endtt

This notation also easily supports operations that refer to themselves, like the
redundant phi functions we have seen for loops:

\begtt \adef!#1{$_{\tt #1}$} \adef|{$\phi$}
...
v!2 = phi(v!1, v!2)
...
\endtt

It should be noted that our notation for values doesn't imply anything about the
order of operations---the only theoretically imposed order is the dependencies
among the operations themselves. The references of operations to each other
actually form a {\em direct acyclic graph} (DAG) showing the data
dependencies---{\em data-flow}. TODO figure

\label[sec:ssa-deconstruction]
\sec SSA deconstruction

SSA dekonstrukce potřeba někde

Problém kdy a jak (copy instrukce v SSA middle end IR? (nejsou SSA), nebo
naopak backend IR v SSA i s phi nody, to ale zase špatně podporuje to, že
některé instrukce prostě SSA neumožňují - two address code, binární operace,
`setcc` instrukce, je to řešitelné, ale...)

Swap problem, lost copy problem \cite[Briggs1998]

Problém nejen korektnost, ale i rychlost (spousta kopií škodí)
\cite[Boissinot2009].

Zbytek SSA dekonstrukce nečíst, velmi hrubý začátek:

No common processor provides $\phi$-functions on which the SSA form relies. Thus
replacement of $\phi$-functions with other instructions having same effect is
needed. This is step is called {\em SSA deconstruction}.

Since $\phi$-functions are the means of achieving single static
assignment, necessarily eliminating them means that the program will no longer
be in SSA form. Being able to multiply assign a variable.

$\phi$-function's semantics say that it evaluates to whichever values is
appropriate to the control flow happening at runtime. In a representation based
on control flow graphs, this is made explicit by basic blocks

We can achieve the effect of a $\phi$-function with multiple assignments through
a copy instruction in each of the predecessor blocks. TODO figure.

This is the original way of eliminating $\phi$ functions and was introduced
by~\cite[Cytron1991]. Though seemingly simple, there are multiple subtle
problems, which have demanded improvements in this area.

One of the problems is, that there may be no predecessor, that is suitable for
the copy. This can be seen on the following program in SSA form:

\begtt
TODO
\endtt

Inserting copies in to predecessor blocks would look like this:

\begtt
TODO
\endtt

But the problem is that copy inserted into block Z for deconstructing the
$\phi$-function X in block Y takes effect even in case the execution doesn't
actually go to block Y, but goes to block W instead. The problem stems from the
fact, that control flow can transfer from block with {\em multiple successors} (where
execution continues in only one of the successors) to a block with {\em multiple
predecessors} (where $\phi$ functions may be needed to merge multiple reaching
definitions). In a control flow graph where possible transfers of control flow
across blocks are embodied by edges, this is known as a {\em critical edge}.
It is possible to split a critical edge by replacing it with an intermediate
block. The block will have only a single predecessor (the block with multiple
successors, block Z in our example) and a single successor (the block with
multiple predecessors, block Y in our example):

\begtt
TODO
\endtt

Now the copies can be inserted into the new basic block and are executed only
when control flow actually goes from block Z to block Y:

\begtt
TODO
\endtt

Another problem with eliminating $\phi$-functions correctly is that semantically
it is as if all $\phi$-functions in a block executed in parallel at the time the
control flow enters the block. This is due to the fact, that $\phi$-functions
evaluate to the right value based on control flow alone. But copy instructions that
we use for replacing $\phi$-functions execute sequentially. With multiple
$\phi$-functions in a basic block and certain dependencies a naive insertion of
copies into predecessor can lead to incorrect translation.

Briggs~\cite[Briggs1998] identified two examples where this happens, the first is
known as {\em lost-copy} problem and occurs when deconstructing SSA in code like this:

After SSA deconstruction:

\begtt
TODO
\endtt

A copy is missing. TODO. But the example actually has an unsplit critical edge
and splitting it fixes the problem:

\begtt
TODO
\endtt

Though this demonstrates that splitting critical edges can be fairly expensive
in loops where it introduces additional unneeded jumps. The lost-copy problem
can be prevented in a much simpler way:

\begtt
TODO
\endtt

The second example identified by~\cite[Briggs1998] is the swap problem and
can be demonstrated on this example:

\begtt
TODO
\endtt

There are two $\phi$-functions one of which depends on the result of the other.
Naively inserting the copies fails to preserve the right semantics of
$\phi$-functions:

\begtt
TODO
\endtt

Additionally, splitting the critical edge doesn't help:

\begtt
TODO
\endtt

Both problems are ultimately due to dependencies among the $\phi$-functions and
their arguments. Bad order of copies can overwrite needed values. Simplest
possible solution is to do the copies in two steps: copy all phi node arguments
into temporaries and only then copy from temporaries into the right
destinations:

\begtt
TODO
\endtt

The approach is correct, since the first step is non-destructive (unlike the
naive copy insertion) and in the second (destructive) step only new temporaries
are read, which are not involved in any $\phi$ functions. While this is correct,
it produces a large number of copies, most of which are not needed.

Briggs~\cite[Briggs1998] approaches the problem as a scheduling problem. Since copy
instructions have a destination (definition) and a source (use), we try to find
an order such that all uses of a virtual register precede its definition. If
this is not possible due to cycles (like in the {\em swap} problem), then a
temporary and an additional copy needs to be inserted to break the cycle.

Briggs also makes an important observation---the problems only manifest after
some of the more aggressive operations are run (like copy folding), and not
after others (like copy propagation or dead code elimination).
Sreedhar~\cite[Sreedhar1999] calls the form of SSA constructed
by~\cite[Cytron1991] and other algorithms {\em conventional SSA} and notes that
it has the important property that all virtual registers in the same {\em phi
congruence class} (containing virtual registers connected via
$\phi$-instruction) can be replaced by one representative and the
$\phi$-instruction eliminated. This means that for example the following code
(in conventional SSA):


Can be translated just by \"omitting the versions":

\begtt
TODO
\endtt

Importantly, in~\cite[Sreedhar1999] they also consider {\em transformed SSA},
which doesn't have the {\em phi congruence property} and present three methods
for transforming transformed SSA into conventional SSA. The three methods
actually build on top of each other and successive methods make use of liveness
(described later in section~\ref[liveness]) and interferences (also described in
a later section, \ref[interference]), but the first method is remarkably
simple. In addition to the copies in predecessor blocks a copy of the
$\phi$-instruction is added. But these are different copies than in previous
algorithms! Here, we are still in the realm of SSA, so these copies can actually
be considered {\em aliases} for the original values and they obey the single
assignment property:\fnote{In fact, exactly because these SSA values are aliases, they
are folded by algorithms like {\em copy folding}, which are the ones producing
the transformed SSA form, which exhibits problems with naive SSA
deconstruction.}

\begtt
TODO
\endtt



Full SSA deconstruction based on Sreedhar's approach thus consists of
conventionalization of the SSA form, merge of virtual registers in the same phi
congruence class (\"drop of versions of variables") and simple deletion of the
$\phi$-instructions.



Since the problems stem from the fact that 

Briggs describes extensions to SSA deconstruction that are able to correctly
translate the two examples, but 

These two examples demonstrated that naive SSA deconstruction as described
by~\cite[Cytron1991]. 


For example, this program in SSA form:

When $\phi$-functions are naively eliminated with copies we get:

But the problem is that although the 


phi functions in parallel

lost copy

swap






A lot of intermediate representations may use single static assignment and
$\phi$-functions. The

Since the single static assignment property of SSA can be achieved in any
intermediate representation, they can have different other needs apart from
replacing $\phi$-functions. 

For example in value based SSA form it may be
desirable to also 

Replacing $\phi$-functions is not the only thing SSA deconstruction has to deal
with. 

Code in SSA form is not directly executable by any common processor. Not only
because the backing IR representation may not represent actual machine
instructions, but because even if it did, $\phi$-functions 

There are two problems with SSA form that make it problematic to translate to
non-SSA form. First is, that in SSA form instructions represent {\em values}.
While they can be thought of as virtual registers, they are not registers, since
they cannot be assigned. The second problem is that SSA form relies on
$\phi$-functions, which don't have equivalent instructions in any usual
instruction sets (like x86-64). Both problems make SSA deconstruction a mandatory step,
that has to happen sometimes before machine code is emitted.

%This is the first
%disconnect of SSA form from real instruction sets, like x86-64, which operate on
%registers. For example, in SSA form, addition of two numbers looks like this:
%
%\begtt
%v1 = add 3, 4
%\endtt
%
%The name `v1` represents the value, which is the result of operation adding
%numbers $3$ and $5$ together. The equal sign doesn't really represent an
%assignment---what is right to the equal sign represents the value entirely. The
%reason why we use the equal sign is that other operations might want to refer to
%previous values, and in textual notation we need to write that somehow. For
%example, `v1` might be used to produce `v2` like this:
%
%\begtt
%v2 = add v1, 5
%\endtt
%
%Implementations often represent values as \"objects" (instances of classes,
%structs) and operations refer to other values through {\em pointers}. While even
%for implementations it might be beneficial to number values like we do in the
%textual representation (because then a simple array can be used for mapping the
%values to some other secondary information), there is no real semantic reason to
%do so.

So in SSA form values are produced from other values and these values sometimes
represent operations, instructions that execute and compute the values,
otherwise values just represent static objects. In machine code we only have
a few physical registers and instructions which usually read from registers and
memory and also write to registers and memory. To translate SSA form we need to
somehow map the values into registers, translate SSA instructions (value
computations) to machine instructions, and we also need to somehow translate the
static objects and be able to take their addresses.

A powerful abstraction that pretty much all compilers have used is, to pretend
in early stages that there is an infinite amount of registers available, the so
called \"virtual registers". A later stage, called {\em register
allocation}, is responsible for translating the use of unlimited amount of
virtual registers into a fixed amount of physical (machine) registers, which is
available for the particular kind of processor we are targeting. Even though
there are techniques which do register allocation in SSA form and deconstruct
SSA form only after (or during) register allocation, we will focus more on the
traditional approach of deconstructing SSA form before register allocation and
thus we will have the luxury of unlimited supply of (virtual) registers. We will
number the virtual registers from $1$ and we will prefix them with the letter
\"t" (for {\em temporary})\fnote{Using \"v" as \"virtual register" would be
possible, but would clash with our notation for numbering SSA {\em values}.} for
example `t1` represents the first virtual register.

With the constraints given above, translating SSA to something like a three
address code is relatively straightforward. The first register in three address
code is the destination, where the result of the computation is written, while
the other two registers are the sources, the arguments of the instruction. Each
SSA value (which is an operation) can thus be assigned its own virtual register,
which will hold the SSA value and like in SSA it will be assigned statically
just once. But other instructions translated later will be able to refer to
it through that virtual register. So something like:

\begtt
v3 = mul v1, v2
v4 = div v1, v2
\endtt

Can be translated to:

\begtt
mul t3, t1, t2
div t4, t1, t2
\endtt

For readability purposes, we used one-to-one mapping from value numbers (used
for the textual notation of SSA form) to virtual registers. This makes it easy
to follow by human, but the compiler itself doesn't have to do it this way, it
just has to ensure to use a consistent mapping of values to virtual registers.
The mapping is not as straightforward as it might seem though, because 

Another so far neglected aspect of SSA form are the $\phi$ instructions. No real
processors offers something like $\phi$ instruction, so we have to translate
them specially.

Important question is also {\em when} do 


Since no real processor implements something like a $\phi$ instruction, which 

\sec Instruction selection

Dva hlavní přístupy tiling (tree, DAG, dynamic programming, bottom up rewrite,
LR parsování, ...) a peephole optimalizce (RTL, expanze a komprese)

Oba přístupy v podstatě to samé, jakmile peephole přestane pracovat s
instrukcemi, kterou jsou po sobě v CFG, ale začne využívat dataflow

\secc Tiling

Základní idea

Asi rychlé představení pár algoritmů tilingu (DP, LR, BURS)

Potřebuje DAG nebo tree.

Nevýhody - instrukce s více výstupy.

\secc Peephole

Původní návrh Fraser a Davidson. Jejich cíl bylo mít jednoduchý generátor
naivního kódu pro každou architekturu, univerzální \"register transfer"
reprezentaci instrukcí a machine description, který jeden stejný peephole
optimizer interpretoval (jako stringy). Každá dvojice a trojice nahrazena za
nejoptimálnější sekvenci. Výhody - žádná case analysis v základním code
generatoru, ani nikde jinde - peephole optimizer bere informace z machine
description. Problémy - založeno na interpretaci stringů, v podstatě dělá
abstract interpretation na všech instrukcích.

Další jejich vylepšení: expander, cacher, combiner. Uvažování logicky
následujících instrukcí (dataflow). Vygenerování patternů na
základě běhu s abstraktní interpretací na nějaké sadě programů ("rule
inference").

Pokud je stejná reprezentace použitá i pro register allocation, lze znovu
spustit jako cleanup fázi i po register allocation


\label[ragalloc]
\sec Register allocation

Register allocation is the last of the three big conceptual parts of a usual
compiler backend. Motivation, importance and possible approaches are introduced.

The x86-64 architecture (see \cite[ref:x86]) is what we mainly care about in
this thesis. Since it is familiar, we will be using it as examples in the
following sections. It is also a good candidate because it brings some
challenges not found on other architectures, but shows the general problems just
as well as other architectures.

Note that like with instruction selection although we are already working with
target specific instructions, their form doesn't necessarily have to be target
specific. Target independent representation of target specific instructions
allows us to share also register allocation logic for all targets.

\secc Motivation

Most CPU architectures these days are register based. That means that interface
of the CPU consists of a fixed number of registers and instructions that allow
operations on these registers. For example registers may be eight 32-bit storage slots
and instructions may allow performing arithmetic on these registers or allow
loading/storing contents of register from/into memory. Memory is still an
important part of these architectures---computations can't possibly fit all into
a fixed number of registers of fixed size, it is the memory that allows us to
store large amounts of data.

During previous phases of the compiler we used a powerful abstraction, we
pretended that there is an infinite amount of registers. This is very important
for the middle end IR, since it is supposed to be platform agnostic and rather
than limiting to some fixed number of registers (per architecture or wholesale),
we might as well pretend to have infinite amount of them. But once we start
translating the middle end IR we just need to limit ourselves to fixed amount of
registers somewhere.

After instruction selection (which determines what instructions to use) and
instruction scheduling (which refines the order of the instructions), a snippet
of input to register allocation can look as follows:

\begtt
mov t1, 1
mov t2, 2
mov t3, t1
add t3, t2
\endtt

%The snippet could correspond to this middle end IR:
%
%\begtt
%v1 = add 1, 2
%\endtt
%
%The translation to instructions given above is suboptimal and in a reasonable
%compiler such code wouldn't get as far as to the register allocator: middle-end
%could fold the addition of constants into a constant, or instruction selection
%could take advantage of the \"register plus immediate" instruction for the addition.
%Nonetheless the example serves us well for showing how a simple allocation of
%registers might look like.


There are several things of note here. The instructions don't operate on real
machine registers (like `rax`), but on {\em virtual registers} (often also called
\"pseudoregisters" or \"temporaries"). It is the goal of register allocation to
transform the code so that {\em physical} (\"machine") registers are used.
Since the whole program doesn't use more than 16 registers, we have no problem
assigning x86-64 registers directly, for example in the order of the
temporaries:

\begtt
mov rax, 1   // t1 = rax
mov rbx, 2   // t2 = rbx
mov rcx, rax // t3 = rcx
add rcx, rbx
\endtt

Even for such a simple example, we can notice several things about register
allocation alone:

\begitems
 * We introduce a third register `rcx` to store the result of addition. This
works well and fits into the 16 registers we have available. But we can notice
that after the addition we no longer need the value stored in register `rax`.
This is on of the big ideas in register allocation, we only need to store those
values that will be needed in the future, and we can use that to \"reuse"
registers.

* If we were to reuse `rax` for storing the result of addition, our situation
would look like this:

\begtt
mov rax, 1   // t1 = rax
mov rbx, 2   // t2 = rbx
mov rax, rax // t3 = rax
add rax, rbx
\endtt

Move (copy) of a register to itself is a no-op, the instructions doesn't have
any real effect. It doesn't even change flags, to it is safely possible to
remove it. We can notice that the two address code generated
from SSA three address code can be improved if it turns out that the
destination can be the same register as the first source (or the second source
in this case, since addition is commutative).

Though this brings a question to which we will come back later: Can the register
allocator remove the instruction? Does it have sufficient information to do so?
Or should it even be concerned about the semantics of instructions it is working
with?
\enditems

While we have shown that opportunities for register reuse arise, it doesn't mean
we can't get out of registers. After all, there is a limited number of them, and
opportunities for reuse come only when a virtual register is no longer needed
later. Even relatively simple expressions can produce code which requires
surprising amount of registers, while not allowing much reuse. For example,
compilation of the expression `1 + (2 + (3 + 4))` can produce code like the one
below:

\begtt
mov t1, 1  // t1 = rax
mov t2, 2  // t2 = rbx
mov t3, 3  // t3 = rcx
mov t4, 4  // t4 = rdx

mov t5, t3 // t5 = rcx
add t5, t4

mov t6, t2 // t6 = rbx
add t6, t5

mov t7, t1 // t7 = rax
add t7, t6
\endtt

Possible register allocation is noted in the example. The right associative
nature of the expression means, that for each of the additions while the left
hand side (the immediate numbers) are evaluated first, its result has to be kept in
registers until the right hand side is evaluated and ready for the addition.
Register reuse is possible but only after the additions, because each has at
least one argument that is not needed further. The example could be extended to
exhaust all available registers. Contrary to that the left associative version
(i.e. `((1 + 2) + 3) + 4`) needs just two registers:

\begtt
mov t1, 1  // t1 = rax
mov t2, 2  // t2 = rbx
mov t3, t1 // t3 = rax
add t3, t2

mov t4, 3  // t4 = rbx
mov t5, t3 // t5 = rax
add t5, t4

mov t6, 4  // t6 = rbx
mov t7, t3 // t7 = rax
add t7, t5
\endtt

Since the operands are kept in registers only for short time in between the
additions, there are more possibilities for reuse. So even though both versions
use the same amount of {\em virtual} registers, they need different amounts of
{\em physical} registers. While instruction scheduling, or perhaps instruction
selection or middle-end optimizations can transform the right associative
version to the left associative one, or just fold the computation entirely
(since it's a sum of four constants), the example illustrates that virtual
register which are needed for a long time are a problem, since they prevent
register reuse and that because of that even simple examples can get out of
registers.

\label[sec:regalloc-spilling]
\secc Spilling

We have to make all values in virtual register available wherever they are
needed. But there may be too little of {\em physical} registers to do so. One
possibility of reducing the {\em register pressure} is to use memory.
In the simplified view of a compiler, there is essentially infinite amount of
memory available, so storing values does not deplete a limited resource as much
as using physical registers does.

%Memory
%isn't all-saving, since computations usually must involve at least some
%registers, but we can use memory to store values temporarily---between places in
%which the values need to be present in registers.

Techniques that involve using memory to reduce register pressure are usually
called {\em spilling}---alluding to the fact that what does not fit into
physical registers is put somewhere else, and that in fact it is an undesirable
thing and we use it only when absolutely necessary.

The best place for storing spilled values is on the system stack (also often
called \"frame stack" or \"call stack") in the function's call frame. This is
due to the same reasons why it is a good place for local variables---each
invocation of a function gets its own locations for storing the values. This
keeps the functions reentrant and for example naturally supports recursion.

Architectures usually also have a dedicated register for the pointer to the
top of the stack, which means that code needing to access the values not
fitting into registers can address their memory locations using relative
addressing with small offsets, also a feature efficiently supported by all
common architectures these days. On the other hand accessing static slots of
memory would pose similar challenges as accessing global variables does (see for
example section~\ref[TODO] for more details).

\label[sec:use-of-spilled]
\seccc Using spilled values

Putting values into memory brings in the problem of using them in instructions.
The generated code used operations involving registers and often instructions
don't allow memory locations to be used everywhere where registers are allowed
to be used. In fact, on processors employing the \"load-store" architecture
(which is one of the signatures of RISC processors), there are only few
instructions for loading values from memory into registers and a few
instructions for storing register contents into memory and no other instruction
can address memory locations. But, the computation of a value still needs to
store it into a register, just like the use of the value needs it to be present
in a register---though between the definition and use, the value can reside in
memory. To achieve this load and store instructions are introduced. These
inserted loads and stores are called {\em spill code}.

What makes spilling beneficial, is that the registers involved in the spill code
(and in the associated definitions and uses of the spilled values) are only used
very locally. Additionally, the registers used for storing and loading are
essentially completely independent, because each load and store can use a
different register. This makes register allocation much easier
(or even possible), since this essentially introduces new and much less
\"constrained" virtual registers.

%Involving memory in places where access to registers was presumed means adding
%new instructions that perform the loads and store to the memory, these
%instructions are called {\em spill code}. This code is necessary, since many
%instructions require at least one of their operands to be in a register. RISC
%architectures usually employ so called \"load-store" model, where there are only
%a few instructions that are able to load data from memory into a register or
%store register contents into memory. Any computations involving operations on
%spilled values have to still use registers and in order to use a spilled value
%as an operand, it 

%Indeed
%especially on modern CPU architectures using memory instead of registers has big
%negative performance implications.
%So not only is use of registers a must due to 


%One of the
%most common approaches of using memory instead of registers is called 

%Storing some values into memory and then fetching them back helps by making
%more registers available.

%The solution is to put the values into memory. Resorting
%to putting the values to memory is called \"spilling".
%The system stack (also often called \"frame stack" or \"call stack") is best suited for spilled
%values, much for the same reasons it is the best place for storing local
%variables: recursion is handled naturally and well, since each {\em invocation}
%of a function gets its own locations for storing the values---by storing the
%values on the stack, we allow functions to be {\em reentrant}.

%As established, we will refer to spilled values through relative addressing
%(using either the stack pointer or the \"base" pointer, also often called the
%\"frame pointer", see TODO). But we want to use the values in computations and
%processors (mostly) operate on registers. But processors mainly support
%using registers for computations, not locations in memory. This is especially
%true 
%
%RISC processors usually employ a \"load-store" architecture. There are only a
%few dedicated instructions for reading values from memory into registers
%(\"load" instructions) and writing register contents into memory (\"store" instructions).
%All other instructions operate only with registers. This means that to even have
%a value to spill, it has to be in a register! Also only from that register, we
%can store the value to memory and later retrieve it, but again only into a
%register. The important observation here is, that resorting to storing the
%values in memory doesn't in general mean that we get around of doing register
%allocation. What we gain is that instead of having to store a value in a
%register for a significant part of the code, by putting the value into memory and
%retrieving it immediately before each use, we have made the register allocation
%problem simpler---a register is needed in {\em more} but {\em smaller} parts of
%the code. This means that a single machine register can be reused for virtual
%registers instead of being blocked by one.

%At least the register used for the store, doesn't have to be the same one used
%for the load. So even though we are constrained by the fact that registers have
%to be used for spills, we are not too constrained in choosing the registers for
%performing spills. Loads and stores inserted for handling spills are usually
%called the {\em spill code}.

In this text, we care especially about the x86-64 architecture, which, like most
CISC architectures, doesn't have a load-store architecture. There are
for example instructions, which can perform arithmetic directly on locations in memory.
Though at most one operand of an instruction can be a location in memory, the
other one has to be either a register or an immediate value. So on one hand, the
problem of having to use registers for storing/retrieving spilled values
remains, but on the other hand, since one operand can be a location in memory,
we can take advantage of that and not use an intermediate register at all.

Even though the x86-64 architecture allows
the instructions to operate on memory operands, an implementation of the
architecture in for example some new Intel processors splits these instructions
into micro-operations based on the load-store architecture, using internal
{\em architectural} registers (not accessible directly) for storing the
intermediate values. Because of this there probably isn't any {\em direct}
performance difference of using the memory operands. But it is still very useful
to use these more complex instructions---not only can the instruction encoding
be shorter (thus sparing the instruction cache of the processor), but we take
advantage of the internal architectural registers, that we normally wouldn't
have access to, which can mean less constraints on the use of the ordinary
{\em physical registers} that we have access to, which may allow storing more
values into registers instead of memory and hence have a significant {\em
indirect} impact on performance.

For example, suppose that `t3` needs to be spilled in the following:

\begtt
add t3, t2
\endtt

The straightforward solution is to add a load before and store after:

\begtt
mov t4, [rbp+s3] // s3 = an offset to t3's spill stack slot (immediate integer)
add t4, t2
mov [rbp+s3], t4
\endtt

We have to be careful about actually inserting loads and stores, because `t3` is
both {\em used} and {\em defined} in this instruction---the instruction
essentially does `t3 := t3 + t2`. Because of this, the register used for loading
`t3` from memory is the same one that will hold the result that needs to be
stored back into memory, so the store needs to use the same virtual register
the load does, here it as `t4`.

This example shows, that as mentioned, at least with a very narrow local
view, and with the first straightforward solution, spilling `t3` doesn't help with the
use of registers. We introduced another pseudoregister, `t4`, to substitute
`t3`, but the original instruction just became surrounded by memory operations.
Indeed, spilling helps only in a broader scope, where for example `t3` had more
definitions and uses.

We can alternatively just operate on the memory location:

\begtt
add [rbp+s3], t2
\endtt

This seems beneficial, since the processor will likely do the same three fetch modify
write operations we had with with our own spill code, it will use an
architectural register to do so, and we don't need any physical register
(represented by above by `t4` virtual register) to do so. But we shall look at
this in bigger context than a single instruction. The x86-64 two address code is
often generated from three address code (likely from SSA form), for that the
code generator likely had to introduce a copy to preserve the value of the first
operand\fnote{Technically, the lowering step doesn't have to generate the copy
to preserve the value if it is not needed. But, as mentioned further, this
requires non-trivial liveness analysis and eliding (coalescing) the copies is
what register allocators try to be good at, so emitting the copy unconditionally
is reasonable solution.}, so in fact assuming that the original IR was:

\begtt
v3 = add v1, v2
\endtt

the full x86-64 code would be:

\begtt
mov t3, t1
add t3, t2
\endtt

Now the prospect of naively spilling `t3` seems even worse:

\begtt
mov [rbp+s3], t1
mov t4, [rbp+s3]
add t4, t2
mov [rbp+s3], t4
\endtt

The code generator hoped that by assigning `t1` and `t3` the same register, the
move instruction could be eliminated. Since for some reason `t3` was spilled,
that is now out of the question, but there is still a suboptimality---`t1` is
copied to memory and then immediately loaded back again into `t4`, because it
needs to be used in the `add` instruction and then also stored back into memory.
Use of `t3`'s memory location as the first operand of the `add` instruction
helps:

\begtt
mov [rbp+s3], t1
add [rbp+s3], t2
\endtt

But the issue is just hidden---the memory operations are still
there, the CPU still has to load the value `t3` from memory in the `add`
instruction, when we just had it in a register. Just because the addressing
modes of x86 allow use of memory locations in the instruction encodings, doesn't
mean that internally the ALU (Arithmetical logical unit) can suddenly operate
directly on memory, the CPU still has to internally load the value into a
register. So while we spare a general purpose register and instead use an
architectural one, we still excessively operate on memory.

Alternatively, starting from the straightforward 4 instruction sequence, we can
just forward the load and use `t1` to populate `t4` directly:

\begtt
mov [rbp+s3], t1
mov t4, t1
add t4, t2
mov [rbp+s3], t4
\endtt

Now the dead store to `[rbp+s3]` is even more apparent, and can be optimized
away:

\begtt
mov t4, t1
add t4, t2
mov [rbp+s3], t4
\endtt

Now, we keep the values in registers the whole time, and only store the final
result in memory on assumption that later the value is needed and storing it in
memory somehow helps the register allocator. While the first example is two
instructions, it is X bytes, while the second one is 3 instructions and Y bytes
TODO: compare lengths of instructions.

Another important observation is that, the \"optimal" code we ended up with, is
very similar to what we started with---there is just a store instruction in the
end. Whether spilling `t3` helped is impossible to tell from this context. But
we can something about the possible benefits of spilling `t4`---there are none.
By spilling `t4` we would get the same instruction we already have, just with a
different pseudoregister (say `t5`). We are at the `t3` spilling {\em fixed
point}. For this reason we should prevent the register allocator from thinking
that spilling `t4` might be a good idea, since it might lead it to an infinite
loop. The reason why spilling `t4` is not beneficial is the fact that the potentially
inserted loads and stores are redundant. The more general pattern is, that any
definition immediately followed by use is not a possible spill candidate. This includes
virtual registers inserted for spill code (so prevents potential infinite loops
spilling spill registers), but can also prevent spilling other virtual
registers, which otherwise might look like plausible spill targets, but in fact
aren't.

A different point of view is, that essentially, in (the last snippet)
in the first two instructions `t3` is represented by `t4`, while in the last
instruction it shifts back to being represented by `t3`, which due to some
previous decision, resides in a memory location `[rbp+s3]`. It is as if we have
originally split the register `t3` into multiple registers (`t3` and `t4`
connected by moves) and only spilled `t3`. It results in the same great code as
our optimized spilled version improved the generated code, because some of the
multiple registers can be much less constrained than the original one and are
thus less likely to be spilled and more likely to get assigned a physical
register.

In contrast, we argued, that by merging `t4` and `t1` we would have had the
chance to eliminate the move instruction, thus improving the code as well. Both
"merging" (called {\em coalescing}) and \"splitting" (called {\em live range
splitting}) can improve the code in different situations, this makes it
even harder, because recklessly doing one or the other will make the code
certainly worse, while doing neither may be just as bad. This makes great register
allocation even harder.

\seccc Interaction with instruction selection

As we have seen, the process of spilling needs to insert new instructions which
together form the so called "spill code". In simplest scenario they just load
and store the value, but better code can be achieved if the spill code is
inserted with more thought than just load from memory and store to memory. But
both of these problems---selecting the instructions to use for operations and
choosing best ones in the current context is exactly the job of instruction
selection.

In principle, register allocator shouldn't care about the instructions. It
should only care about their effects on the registers. The result of register
allocation should be the assignment of register to virtual registers. Since spills
can be necessary, which requires insertion of new instructions, we have to
decide how this spill code will be handled. The basic options are the following,
and mostly depend on the chosen register allocation technique:

\begitems\style n
* Insert spill code in the register allocator pass.
* Return the list of spilled virtual registers, and expect to be called again with
code transformed to include the spill code.
\enditems

The first option can make the register allocator depend on the target
architecture---it now needs to know about the current target, its instructions,
their meanings and how to insert them. On one hand, register allocation is
already in the "backend", where we expect to handle things on the level of the
target, and generally hope to take advantage of that by, for example, doing
optimizations specific to the particular target. On the other hand, as mentioned
previously, machine independent representations of (machine dependent)
instructions are possible, thus spill code insertion {\em could} also be made
machine independent similarly just like the register allocation.

The second approach also makes the register allocation process pure in a sense. The register
allocator never modifies the input, its result is a either a mapping of
virtual registers to physical registers, or a list of virtual registers to be spilled.
Though it still has to be decided on how to insert the instructions. Some
instruction selection mechanisms which ultimately depend on the middle end IR,
are not suitable for inserting and optimizing spill code, since we are already
in the low level backend IR. Tree or DAG based instruction selection
mechanisms may also not be directly applicable---we may no longer be using
trees or DAGs for representing the machine code, especially after instruction
scheduling, which sets the order of instructions in stone. On the other hand
peephole optimization is a great fit for improving inserted spill code. The
inserted spill code can be very naive, and peephole optimization run on the
spill code and its surroundings can make improvements. This is especially likely
if we are able to find patterns which spilled code creates, such as in the
example in the previous subsection (\ref[sec:use-of-spilled]).

The obvious downside of the \"pure register allocation" approach is, that it has
to start over with register allocation if any spills need to be made. The first
approach seems more suited to register allocation where we want a self
contained fast single pass register allocation.

The approaches used for spill code insertion are usually very connected to the
core principle of the register allocation algorithm at hand, and choices of some
approaches are discussed in section~\ref[sec:regalloc-techniques].

\secc Formalization

The terms used in the previous sections about register allocation were not
properly defined, and used a bit vaguely. The intention was to practically show
the problems register allocation tries to solve and what it needs to do to solve
it, which includes optimizations that try to make the register allocation
process more optimal. In this section, we try to more properly introduce the
terms used for concepts connected to register allocation.

One thing that has to be noted is the name \"register allocation" itself. We
mentioned that register allocation is meant to map virtual registers to
physical registers of the target architecture, but the process isn't always
so direct, and sometimes it makes sense and brings benefits to split this
process into two parts, which really define what we mean by these names:

\begitems\style n
* {\em Register allocation}. In a narrower sense, by register allocation we mean
the process of making sure that each virtual register can be assigned a physical
one. At this point, we may not care too much about which one, but we care about
spill code, because that is what allows us to fit into the limited amount of
registers available.
* {\em Register assignment}. Assignments follows allocation---now that we
can map every virtual register to at least one register, we choose the
concrete one. Although this seems much more simpler than the allocation part,
in practice register assignment is also very important, because we have seen
situations where some assignments lead to better code, for example where the
source and destination of a move instruction are assigned the same register, the
move instruction can be eliminated.
\enditems

Some register allocation algorithms intertwine both parts and don't split them.
Some algorithms strictly separate these concerns. In general simpler algorithms
usually merge both of these parts, while more complex algorithms try to take
advantage of attacking each of those issues separately to reach more optimal
results. But this distinction is of course not definitive.

\seccc Liveness, interference

Already in previous sections we hinted that we may reuse a physical register if
the virtual register occupying has no further use. Liveness is the property that
captures this formally.

A virtual register is {\em live} at a program point if it {\em may} be used in
future.

The definition of liveness captures the \"not used further" aspect that we found
beneficial for register reuse. If at a program point a virtual register stops
being live (becomes {\em dead}) the physical register allocated to it becomes
available. The definition of liveness carefully says \"may be used in future",
because in general it is undecidable whether a virtual register {\em will} be
used. TODO: Rice's theorem, halting problem.

For computing liveness analysis the means of classical data flow analysis may be
used. For each program point we can compute the liveness property on the control
flow edges coming to it ($\hbox{LIVE}_{\hbox{\it in}}$) and control flow edges
coming out of it.

Liveness is the most basic property on which register allocation is based.
Liveness of a virtual register represents whether from a 
Liveness is a property of virtual registers and tells us at which points in the
control flow a virtual register is {\em live}.

\seccc Live ranges

vs value vs variable.

\label[sec:regalloc-coalescing]
\seccc Coalescing

Coalescing in register allocation tries to assign virtual registers a common
physical register. Usually we only care about coalescing where it is beneficial.
In the low level these are the situations which may manifest as copies from one
register to the other---if the two registers are allocated the same physical
register, the copy is redundant and can be optimized out.

In practice this is connected to several high level concepts that generate low
level copies:

\begitems
* Translation from three address code to two address code. In two address code
the first operand is the same as the destination. When translating three address
code to two address code, we need to introduce a copy to not clobber the first
operand in case it {\em lives out}. For example this three address code
instruction:

\begtt
sub t1, t2, t3
\endtt

Can be translated to:

\begtt
mov t3, t1
sub t3, t2
\endtt

* SSA deconstruction (section~\ref[sec:ssa-deconstruction]). Effects of $\phi$
instructions are usually modelled with copies in predecessor blocks. For example
the following instruction in block 3, which merges virtual registers `t1` and
`t2` into virtual register `t3`:

\begtt
phi t3, t1, t2
\endtt

May translate to the following instructions in the two predecessors blocks:

\begtt
// predecessor 1
mov t3, t1

// predecessor 2
mov t3, t2
\endtt

Note that just these copies may be insufficient in some situations, see
section~\ref[sec:ssa-deconstruction].

* Live range splits. A virtual register can be split into multiple virtual
registers, and copies are introduced to connect them. These can include splits
due to register constraints (including calling conventions). See
section~\ref[sec:regalloc-splitting] for more details.

As coalescing is the exact opposite of live range splitting, virtual registers
that are the result of live range splitting should not be carelessly coalesced,
since that would practically undo the splits.

%* Register constraints. Sometimes instructions require their arguments to reside
%in specific registers or put the results into specific registers or for example
%calling conventions can require arguments to be passed in specific registers. If
%our register allocator is not able to do live range splits on demand  we may want to 
%
%* Callee saved registers.
%
%
%This also
%applies to calling conventions which say which arguments are used for passing
%functions, and which are caller-saved or callee-saved.
\enditems

The removal of the move instructions itself is a simple task that can be left to
the peephole optimizer. The real role of coalescing in register allocation is
to try to allocate move related nodes to the same register. But too much
coalescing can be counterproductive---choosing to allocate two virtual registers
to the same physical one means that effectively the registers are combined into
one. This means that their interferences add together and it becomes much harder
to find a common register.

\label[sec:regalloc-splitting]
\seccc Live range splitting

Live range splitting is essentially the opposite of coalescing. By splitting
one virtual register into multiple, we hope to achieve better register
allocation, since the split virtual registers can be allocated separately and
they can be much less constrained (have less interferences) than the original
virtual register.

A big disadvantage of our model of register allocation being a mapping from
virtual register to physical registers is that a virtual register can be
assigned only one physical register. Live range splitting essentially remedies
it, because the splits are allocated separately.

Another advantage is that, in the simple spilling implementation, every use and
definition of a register is replaced accordingly by a load or store.

Naive spilling of an entire virtual register is simple to implement, but doesn't
consider the nature of the register's use. For example the virtual register may
see significant use in the beginning of the procedure and then in the end of the
procedure and not be used much otherwise. Such virtual register will interfere
with practically all other virtual registers, since it is live during the whole
function. Splitting it to three registers (beginning, middle and end of the
procedure) not only allows different physical registers to be allocated to it in
the different parts, but it also allows {\em independent} spill decisions. For
example, it can be spilled during the middle of the function cheaply with only
one load and one store. The splits here would mean splitting say `t10`:

\begtt
...
mov t10, ...
...,  t10

[...] // a loop

..., t10
\endtt

To `t11`, `t12` and `t13`:

\begtt
...
mov t11, ...
...,  t11

mov t12, t11
[...] // a loop
mov t13, t12

..., t12
\endtt

Splits can be especially beneficial around loops, since spills of any values
used inside loops are costly. Even a lot of shuffling of registers and memory
can be beneficial if a loop is executed many times. Moving expensive operations
out of the loops has been the task of code motion in the middle-end and
instruction scheduling in the backend. Since register allocation generally
follows them, we should do our best to not hinder it.

If the splits turn out to be unnecessary, they can be coalesced away. Though
introducing too many splits can reduce the chance of coalescing in
general---coalescing has to be careful about splits in order to not undo them
carelessly, additionally many register allocation techniques avoid excessive
coalescing.

Spilling can be seen as a very primitive form of live range splitting, which
introduces a new virtual register for each use and definition and spills all of
them. Splitting the uses and definitions manually, and spilling them only when
needed may prove to be better, since even in case the copies across the splits
don't get coalesced, they are presumably cheaper than touching the memory.

Some allocators are able to rethink the register allocation of a virtual
register at its every use or definition. This means that the algorithm can
essentially perform live range splitting everywhere. Other algorithms are not
able to do splitting \"online" and require it to be done as a preprocessing
step, in that case live range splitting can be even more important.

\seccc Register constraints

Some instructions require the arguments to be in particular registers or produce
values in particular registers. Typically, there are two such constraints:

\begitems\style n
* One concrete register has to be used.
* One register belonging to a particular class can be used.
\enditems

Register classes are discussed in section~\ref[sec:regalloc-classes]. Here we
discuss only constraints which require one particular register.

The x86-64 architecture has two notable examples where concrete registers have
to be used for operands or results:

\begitems\style n
* {\em Shifts}. Shift instructions where the shift amount is not an immediate
value, require the shift amount to be in the `cl` register (that is, the low 8
bits of the `rcx` register).
* {\em Long multiplication and division}. On x86-64 the only available
division instructions (for signed and unsigned division) divide a 128-bit number
by a 64-bit value.%
%
\fnote{As with other instructions on
x86-64, the 32-bit variants are available. So for example, the long division
divides 64-bit number by a 32-bit number, but it still operates on two registers
(`eax` and `edx`).}
Upper 64-bits of the 128-bit dividend are expected in `rdx`,
the lower 64-bits in `rax`. The division instructions store the remainder in
`rdx` and quotient in `rax`. The divisor can be a (64-bit) register or memory
location.
\enditems

But there are restrictions imposed not necessarily to the instruction set
itself, but by {\em calling conventions} of the platform. For example on x86-64
Linux the System V ABI prescribes that the registers `rdi`, `rsi`, `rdx`, `rcx`,
`r8` and `r9` are used for passing parameters and `rax` and `rdx` are used for
return values.

Register allocator needs to conform to these requirements. But if the physical
registers are allocated for the entire lifetime of the constrained virtual
registers, then there can be conflicts if there are multiple such constrained
contexts and the virtual registers interfere. For example, if there are the
x86-64 shift instructions:

\begtt
sal t1, t2 // t2 needs to be allocated to cl
sar t3, t4 // t4 needs to be allocated to cl
\endtt

Here `t2` and `t4` both need to be allocated to `cl`, but they are live at the
same time (interfere), so they cannot be assigned the same register. This is the
case regardless of the number of available registers, so this is a problem of
the {\em register assignment} part of register allocation. Spilling can save the
situation, for example spill of `t4` helps if `t2` isn't needed after this
snippet of code:

\begtt
sal t1, t2 // t2 = cl
mov t5, [rbp+s4]
sar t3, t5 // t5 = cl
\endtt

Spilling both of course also helps, but spill of `t2` alone doesn't, because the
newly introduced temporary (`t5`, constrained to `cl`) would still be alive at the
same as `t4` and thus they would interfere:

\begtt
mov t5, [rbp+s2]
sal t1, t5 // t5 = cl
sar t3, t4 // t4 = cl
\endtt

The problem with spilling here is, that it is mainly used as means of reducing
register pressure. Because of that spilling heuristics are meant to spill
virtual registers that are e.g. not going to be used for the \"longest" (which
frees a physical register for the longest time) or which interfere with a lot of
other virtual registers (so the other virtual registers have much higher chance
of being allocated themselves). Nothing usually makes constrained registers good
spill candidates, and it shouldn't, since because of the constraints the virtual
registers {\em need} to be assigned.

Live range splitting is a much better choice for handling constrained registers.
By introducing a new virtual register (and a copy to it) for the short time of
the constrained use, the constrained use doesn't have any chance of interfering
with other constrained uses and is so short lived that it even isn't a plausible
spill target (see section~\ref[sec:regalloc-spilling]). In our example, it looks
like this:

\begtt
mov t5, t2
sal t1, t5 // t5 = cl
mov t6, t4
sar t3, t6 // t6 = cl
\endtt

Here indeed `t5` and `t6` don't interfere and are unspillable. If, like we
assumed earlier, `t2` doesn't {\em live-out}, then the best assignment would
assign `t5` the `cl` register (low 8 bits of `rcx`), like this:

\begtt
mov rcx, rcx
sal rax, cl
mov rcx, rdx
sar rbx, cl
\endtt

Here it was possible to keep all values in registers. The first copy can be
easily optimized by subsequent peephole optimization pass.

Live range splitting is good solution for making register constraints not
constrain the register assignment much. The same ideas that apply to live range
splitting apply also here, in particular allocators which are unable to perform
live range splitting on demand should split constrained uses beforehand and
coalescing may be able to remove the copies if it turns out they are not needed.

Calling conventions also dictate the coordination of registers between the {\em
caller} (calling function) and the {\em callee}. Both want to use machine
registers (and ideally all of them), but if callee uses the registers, it
overwrites values the caller has stored. For that purpose registers are
classified as either caller-saved (caller has to save the registers, if it uses
them) or callee-saved (callee has to save the registers, if it wants to use
them).

The fact that a register is caller saved means that when a function performs a
call, it has to pessimistically assume that the callee changes all the caller
saved registers.\fnote{The callee doesn't necessarily change any of the caller
saved registers, but it is allowed to do so. Calling conventions are general,
and don't try to specialize for some cases. They are the interface functions
need to conform to, and allow code produced by different compilers to cooperate
together. But if a compiler is sure that the function doesn't have to conform to
the interface (perphaps because the function is `static` and thus not callable
by from other separately compiled modules), then it can can try to do better by
employing whole module register allocation, which considers register allocation
even across the calls. Often though a much simpler technique helps with calling
convention constraints---not performing any calls at all! Inlining the called
function into the call site means that the registers used be the called function
are allocated as part of the function procedure and the calling convention
constraints don't apply at all. Leaf functions (functions not calling other
functions) are especially good candidates for inlining especially because they
don't impose calling convention constraints themselves.}
This can be modelled like an register constraint on the call instruction. For
example on x86-64, where for example `rax`, `rcx` and `r11` are caller saved we
can make the `call` instruction {\em define} the registers:

\begtt
call f % defines rax, rcx, r11 and others
\endtt

Coincidentally `rax` is at the same time used for passing the return value, so
it would have been defined by the call instruction already. But `rcx` for
example is used for parameter passing and is thus as mentioned above {\em used}
by the call instruction to model that. Adding definition of `rcx` to the call
means, that the caller shouldn't expect the `rcx` register to be preserved by
the caller. While this model works well for {\em correct allocation}, a lot of
registers are caller saved and virtual registers that {\em live through} the
call can not be allocated to them. Callee saved registers are needed in that
situation. But for example on x86-64 there are 9 caller saved registers and only
7 are callee saved, and out of those 2 are usually reserved for special purposes
(stack and base pointer). Having only a few available registers for values
living across calls means very high register pressure, which will have to be
mitigated by spills. Spills are correct and needed here, since if the registers
are reserved for the caller, we don't have any other choice for storing values
than memory. But naively spilling virtual registers everywhere just because of
high register pressure at a call site is not ideal. Once again this is a place
where live range splitting helps---just like splits around loops were useful,
splits around calls can be useful for minimizing damage implied by spilling
virtual registers at every use or definition.

Callee saved registers can be modelled through uses and definitions as well.
Having definitions of callee saved registers at the entry point of a function
and uses at the exit point (return instruction) models the fact, that the callee
saved register has to be preserved for the entire duration of call. This not
only requires physical registers to be somehow represented, but also blocks the
callee saved register for the entire duration of the function. Live range
splitting is again very useful here, because we can introduce virtual registers
for holding the values in callee saved registers during the function, e.g. if we
consider just `rbx` and `r12`:

\begtt
mov t20, rbx
mov t21, r12
[...]
mov rbx, t20
mov rbx, t21
\endtt

This is much better, since the virtual registers can be spilled, which frees up
a callee saved physical registers, which can possibly be used by many (short
lived) virtual registers or perhaps just one virtual register with more uses and
definitions which would be more expensive to spill. The virtual
registers introduced for callee saved registers are in fact ideal spill
targets---there is only one definition and one use, both outside of any loop.
Spilling them could look like this:

\begtt
mov [rbp+s20], rbx
mov [rbp+s21], r12
[...]
mov rbx, [rbp+s20]
mov rbx, [rbp+s21]
\endtt

Here we refer to some abstract \"stack slots". If the stack slots are chosen
well, `push` and `pop` instructions can even be used for realizing those spills:

\begtt
push rbx
push r12
[...]
pop rbx
pop r12
\endtt

Which essentially gets us the code one would use to free up callee saved
registers. But modelling it through register constraints can nicely take care of
only using the callee saved registers when beneficial and so it is more
flexible.

\label[sec:regalloc-classes]
\seccc Register classes

\label[sec:regalloc-techniques]
\secc Techniques

We have already seen a few things that can distinguish different register
allocation algorithms:

\begitems
* Handling of spilling (section~\ref[sec:use-of-spilled]),
* Split or no split of allocation and assignment
(section~\ref[ref:regalloc-formalization]).
\enditems

\noindent But there are others:

\begitems
* Scope: {\em local} vs {\em global} vs {\em interprocedural} vs {\em whole
program} algorithms. Local algorithms operate on singular basic blocks and use
only information local to the basic block to decide on register allocation. The
limited scope makes the algorithms generally simpler and produces worse results
then global register allocation, which allocates registers to whole functions.
Global register allocation is global in the sense that {\em all} basic blocks
are considered at the same time. The analysis is more complex, since it has to
handle control flow. Even techniques for allocating registers across function
calls and whole programs exist. These can be less practical in practice, where
functions may be required to conform to a {\em calling convention}, which
specifies how arguments should be passed in function calls, what registers are
preserved by calls and where will the return values reside. We will not discuss
techniques operating in larger scopes than {\em global} (whole function, all
basic blocks).

* {\em Quality} vs {\em speed}. With no restrictions on time, we ideally would
like to achieve {\em optimal} register allocation. With the right definition of
optimal, it can be possible, but due to the difficulty of the register
allocation, this approach is bound to be too slow (in {\em compile-time}),
although it would produce code that would be fast (in {\em run-time}). In code
compiled ahead of time, we can probably justify spending more time on
compilation to achieve better run-time, since it is expected, that the program
will run for some time and that the investment will return.

On the other end of the spectre we may want a register allocation algorithm that
runs very fast (due to constraints on compile-time), but in that case we can't
expect good results (i.e. code that has fast run-time). This can be interesting
for {\em Just-in-time} (JIT) compilers, where the compile time is part of
run-time and hence it is not possible to spend much time on optimizations,
because it is possible that they wouldn't pay off (though they could, we don't
know ahead of time).

* {\em Control flow sensitivity}. Some global algorithms may completely
disregard the actual control flow of the program and just use (global) liveness
and/or interferences to do register allocation and assignment. But use of
control flow information in an algorithm is likely to steer it to better
results---spills in hot or nested loops are undesirable. Control flow sensitive
register allocation (not assignment) may for example even try to purposefully do
spills or splits before loops to make more registers available in loops.
\enditems

\label[sec:regalloc-top-down]
\seccc Local top-down and bottom-up

Two of the most basic algorithms for register allocation are described by Cooper
and Torczon~\cite[Engineering]. They operate only on single basic blocks, but
form a good baseline to improve upon. Also, surprisingly, their ideas of how to
handle spilling have their equivalents in more powerful algorithms.

Both algorithms investigate uses and definitions of virtual registers inside a
basic block, allocate some map some virtual registers to physical registers and
spill the others. They differ in how they choose which virtual registers to
spill:

\begitems
* {\em Top-down.} In the top-down view those virtual registers which are used
most often (i.e. their number of uses and definitions is the highest) should be
the ones that get assigned registers, others should be spilled.

While this is simple to implement, there are glaring problems. The algorithm is
not able to reuse registers---since it maps virtual registers to physical
registers one-to-one, it is not able to reuse a physical register once a virtual
register becomes dead.

Also, most instructions require at least some arguments to reside in registers,
but it can happen that top-down register allocator spills all used and defined
virtual registers of a particular instruction. To solve this, sufficient number
of registers has to be set aside and not be allocated, they will be used for
realizing loads of uses and stores of definitions of spilled virtual registers.
This of course makes the allocation results even worse, because only a lesser
number of registers are available for allocation, and spills may be
introduced just because some registers are reserved for spill code realization.

* {\em Bottom-up.} In the bottom-up approach instructions are investigated in
order, one by one, and registers are allocated to supply the demand of each
particular instruction. In general, each instruction is an operation with
multiple input registers and multiple output registers. Hence for each
instruction the algorithm ensures that input virtual registers are allocated
into physical registers and allocates registers for output virtual registers.

It may seem, that since the algorithm operates on a single basic block, that each
use of an virtual registers should have a preceding definition, which should be
the one, which allocates register for it, and that allocation of registers for
arguments is not necessary. But it is necessary for handling spills---allocation
of a physical register (whether for input or output virtual register) may find
that none of the registers is free, so it has to choose one of the assigned
registers, and spill it, by moving the value from that register to memory. Later
the spilled register may be needed again, and so it has to be assigned register
again and the previous value has to be reloaded from memory into the new
register. The newly allocated register doesn't have to be the old one. This is a
great advantage of this approach over {\em top-down}, while spilling, it is able
to effectively split a live range and allocate it different physical registers
or memory locations.

Because the algorithm considers each instruction, it is able to much better deal
with machine constraints. For example, if an instruction needs its operand to
reside in a particular register, or puts the result in a particular register,
then the allocator may just forcibly allocate that particular register. One
example of such constraint are that of the shift instructions on x86-64, which
require the shift size to be specified in the `cl` register:

\begtt
shl t1, t2 % t2 has to be allocated to cl
\endtt

If we suppose that `t1` already resides in a register (say `rax`), and `t2` is
already in `rcx` (the register of which `cl` is the lowest 8 bits), then the
allocator doesn't have to do anything:

\begtt
shl rax, cl % t2 has to be allocated to cl
\endtt

If `t2` is assigned say the `rdx` register, and the value in the `rcx` register
is no longer needed, just a copy is sufficient:

\begtt
mov rcx, rdx
shl rax, cl
\endtt

If however, the virtual register which occupies `rcx` (say `t3`) is live after
the instruction, then we need to find it a new register. And if conveniently
`t2` (the shift amount) is not needed after the shift, then we can just reuse
the newly freed `rdx` register by swapping the registers:

\begtt
xchg rcx, rdx
shl rax, cl
\endtt

And so on. Similarly we could deal with platform calling conventions. For
example, if the called function needs arguments in registers `rdi` and `rsi`,
then we might just forcibly allocate them. If the function call doesn't preserve
other registers (like `rax` or `rcx`), these registers should also be forcibly
allocated---though the call-site will not use them for anything, the called
function might, and hence we need to preserve values in them, and using the
\"allocate a register" mechanism, we elegantly also handle the necessary spills.

When a physical register is needed and none is available, one has to be spilled.
Good choice is to spill the virtual register whose next use is the furthest
away~\cite[Engineering]. This is akin to Bélády's MIN algorithm for page
replacement~\cite[Belady1966]. The benefit of the approach is, that if we need
to spill, the register we free up will be available for other purposes for the
longest time, hence it will hopefully prevent other spills.

The bottom-up algorithm has to do two passes over the code---first one to derive
liveness information, second one to actually do the allocation. Liveness
information provides the information necessary for choosing spills.
\enditems

An interesting twist to the bottom-up algorithm described by Mike
Pall~\cite[Pall2009]. He does the allocation in a single pass over the code in
SSA form, though in {\em reverse}. In programs in SSA form SSA values naturally
correspond to live ranges, and reverse order is natural for computing liveness.
Pall's algorithm essentially combines register allocation with the liveness
computation. While iterating in reverse order definitions are processed first,
while uses are processed next, contrary to the bottom-up algorithm. Also
contrary to the bottom-up algorithm, where {\em definitions} was what derived
the assignment, it is the {\em uses} that drive the assignment in this
algorithm. When a first use of a virtual register encountered, it is allocated a
register, and when (the only) definition of a virtual register is reached, then
it is freed. This is often better, since more often the uses are constrained by
machine constraints, so by discovering the uses first, the allocation can be
targeted more easily.

The problem with all these three approaches is that they are too local. Their versions
as presented above work only in a single basic block. Extensions to global
(whole-procedure) allocation are possible by using memory---a simple extension
does register allocation on each basic block separately and all virtual
registers are stored at the end of each block and loaded back at start of a
each block. This still only requires only local analysis, but produces very inefficient
code. It is possible to improve this by performing global liveness analysis and
to store only virtual registers that live-out and to load only live-in virtual
registers. Though at the point where global analysis is feasible, some of the
global register allocation algorithms is probably feasible as well.

\seccc Linear scan

Poletto and Sarkar~\cite[Poletto1999] introduced o called {\em linear scan} register
allocation. It can be seen as an extension of the bottom-up approach described
in the previous section~(\ref[sec:regalloc-top-down]). The canonical version of
the bottom up is able to allocate registers only for a single basic block,
because it depends on many of the linear aspects of basic blocks, such as that
the instructions are ordered, live ranges are also ordered and it can be
determined which is \"furthest away", so that something akin to Bélády's
algorithm~\cite[Belady1966] can be used. Linear scan register allocation extends
the bottom-up approach to perform global (whole procedure) register allocation
by imposing an ordering over the instructions by (globally) numbering them. This
ordering induces a linear sequence, where live ranges can be represented as simple
intervals starting at the number of the first instruction where the virtual register
is live and ending at the number of the last instruction where the virtual
register is live. This essentially makes the procedure into a single \"basic
block" on which something akin to the bottom-up allocator can be run.

But the algorithm described by Poletto instead operates on the live intervals.
The algorithm orders the intervals by increasing start point and iterates over
them. The algorithm effectively iterates over the starts of live ranges, keeping
the set of {\em active} intervals (those whose start is {\em before} the start
of the current interval, and end {\em after} the current interval). For each
encountered interval, those intervals in the active set which end before the
current interval's start are expired and their registers freed, and a new
register is a allocated from the pool of free registers for the current live
interval. If the number of intervals live at some point exceeds the number of
available registers a live range needs to be spilled. Poletto and Sarkar choose
to spill the live range which ends furthest away---if that live range is the one
which is currently being allocated, it is not allocated a register, but a memory
location instead, otherwise the current live range is assigned the register of
the spilled interval and the spilled interval is assigned a memory location.

There are many problems with linear scan. It has been designed as a fast and
simple alternative to graph coloring register allocators
(see~\ref[sec:regalloc-graph-coloring]), mainly for JIT compilers which value
greatly run-time of {\em the compiler} and can sacrifice run-time of the
compiled code. The relatively poor allocation quality is intentional.

The reasons for the poor quality is that live ranges are really really
imprecise, since this simple live ranges don't represent live ranges
truthfully---there may be many instructions in the middle of the interval, where
the virtual register is not live. In fact trivial live range $[1, n]$, where $n$
is the number of instructions is correct for each virtual register, but of
course produces unsatisfactory allocations. Another problem is, that unlike the
bottom-up approach linear scan has more trouble with handling of spilled code as
well as machine constraints. This is because virtual registers (live intervals)
are assigned either a register for their entire duration, or a memory location.
For use of the memory locations in instructions registers have to be used, and a
few registers would have to be set aside for that (like with the top-down
allocator from section~\ref[sec:regalloc-top-down]). Basic form of linear scan
doesn't handle machine constrains at all 

While in some sense linear scan register allocation can be seen as a extension
of the bottom-up register allocator, it suffers from many of the issues of the
top-down allocator. Some of the follow ups on linear scan are much better suited
to practice. For example in~\cite[Traub1998] they are able to additionally deal
with holes in live intervals and can assign a multiple registers to a single
live range (at different times) and other approaches are able to better handle
machine constraints and use properties of SSA form~\cite[Mossenbock2002,
Wimmer2010].

\label[sec:regalloc-graph-coloring]
\seccc Graph coloring

Even though the idea of using graph coloring for register allocation is older,
first notable use of the technique is by Chaitin~\cite[Chaitin1981, Chaitin1982].
The core of the idea is to construct an interference graph from the
interferences of virtual registers---all virtual registers become nodes in a
graph and there is an edge between virtual registers if and only if they
interfere. Then on this graph we aim to find a {\em coloring}---mapping of nodes
to colors, such that no neighbouring nodes get the same color. In our case the
colors ultimately constitute the machine registers. Because edges designate
interference, it is guaranteed that no virtual registers that interfere are
assigned the same physical register. If we have $k$ machine registers available,
then we are looking for a $k$ coloring---the coloring needs to use at most $k$
colors (registers).

By reducing the register allocation to graph coloring we may seemingly not
gain much, since graph coloring is an NP-complete
problem\fnote{In~\cite[Chaitin1981] authors argue further that register
allocation is also NP-complete, this has since been
disputed~\cite[Bouchez2007].}. However in~\cite[Chaitin1981] they rediscover a
technique that can simplify and make graph coloring practical for register
allocation. It is based on the observation that a node which has fewer than $k$
neighbours can be always assigned a color distinct from all of its neighbours.
Because the node has less neighbours than there are available colors, even if
all neighbours used different colors, there would still be a free color left.
This simple, yet important observation is the base for their and derived
techniques.

Since incorporating a node with degree (number of adjacent nodes) less than $k$
(a so called {\em insignificant}
node) into an already colored graph is easy, initially we do the
opposite---remove from the graph all insignificant nodes, such that later, in
the reverse process we can add them back to the graph and color them trivially.
Removing low degree nodes from the graph causes the degrees of neighbouring
nodes to decrease as well and may thus lead to more simplifications. In
Chaitin's algorithm this phase is called {\em simplify} and the
removed low degree nodes are pushed onto a stack. In the final stage, called {\em
assign}, the nodes have colors assigned in the reverse order simply by popping
them from the stack and assigning them a color not used by any of the already
colored neighbours in the now being rebuilt graph. Use of this heuristic is not
all saving---it is possible that after after simplification (removal of low
degree nodes) there will still be high degree nodes left. In that moment, push
of any of the remaining nodes on to the stack, could mean that there won't be a
color left for it. Chaitin's solution is to calculate spill costs of all the
remaining high degree nodes, choose the one with the lowest cost, mark it as
to be spilled and remove it from the graph. Due to the removal, the
simplification process may find more simplifications, otherwise another spill
decisions may be made. The removal of the to be spilled node from the graph
simulates its replacement by loads and stores, which although will introduce new
virtual registers, they will have very short live ranges, with (hopefully) much
less interferences, so it suffices as an approximation.

Spill of any node means that the code needs to be updated with spill code, and
the register allocation process repeated. Since the program is now different,
and new pseudoregisters were introduced to accommodate spill code,
liveness analysis and building of interference graph have to be repeated as
well. Then simplification can be tried again. This entire process is tried
until the simplification is able to reduce the graph to an empty graph, which is
trivially colorable. Since each iteration is very expensive, it is important
that there can be multiple spill decisions made in a single {\em simplify} run,
this way the process can often finish in 1 or 2 iterations, if the first
iteration successfully finds all nodes that need to be spilled and the second
iteration finalizes the assignment.
Being able to spill only one node on each iteration would mean that graphs with
many high degree nodes would need {\em many} expensive iterations to finish.
Proceeding from {\em simplify} only after all spill have been handled makes it
possible to push only low degree nodes, guaranteeing that in the reverse {\em
assign} stage, every node popped from top of the stack will have at least one
free color.

But, it is possible to do better. The fact, that a node has a {\em significant}
number of neighbours doesn't mean, that it will be uncolorable in the {\em
assignment} stage. There is a chance that the already colored neighbours will be
assigned less than $k$ distinct colors (TODO: figure), in that case the popped
node could still be colored even though at the time it was pushed it was
significant. Because of this, in {\em simplify} we may optimistically try to
remove and push high degree nodes on to the stack, instead of pessimistically
spilling them. If in the assignment phase it turns out that there isn't a color
left for the popped node, we spill it only then. We call these pushed high
degree nodes {\em potential spills}, since they become {\em actual spills}
spilled in the {\em assign} stage. This strategy is called {\em optimistic
coloring} and was devised by Briggs~\cite[Briggs1992, Briggs1994]. Even in this
strategy, it can happen that more than one (potential) spill will be necessary.
Like with Chaitin's \"pessimistic" spilling, the best possible node for
potential spill is the one with the lowest spill cost---first potential spill
will be processed last in the {\em assign} stage, and will encounter a more
complete interference graph, than potential spills pushed later, which are
assigned colors earlier. Like before, we want to capture all {\em actual}
spills, before we repeat the whole process with spill code inserted. To do this,
in the {\em assignment} stage we don't allocate the actual spills any color,
just mark them for spilling and proceed. This can make more some nodes
neighbours of actual spills colorable, since by not coloring the actual spill,
they effectively have one less interfering node. Like with Chaitin's spilling in
{\em simplify} stage, this approximates the actual effect of spilling, which
splits a single node into many temporaries whose interferences are more local
and hopefully easier to deal with.

As we have seen before (in section~\ref[sec:regalloc-spilling]), spilling can always be
necessary, reducing the register allocation problem to graph coloring doesn't
change that. No matter how $k$ is big, there are always graphs which need more
registers to be colored successfully. Even an exact graph coloring algorithm
that tries all possibilities can fail to find coloring because of this. Spilling
is thus {\em not} only due to Chaitin's heuristic. Though the heuristic even
with Briggs' optimistic coloring can introduce more spills than a more exact
algorithm would.

The interference graph is a really great data structure, because apart from
being able to represent the \"live at the same time, and thus unallocatable to
the same register" constraints, it can express also other restrictions. Machine
constraints and calling
conventions can both be
modelled by interferences (edges in the interference graph) if we also add
physical registers as nodes. For example, we can force a virtual register node
to be allocated to a particular physical register by making it interfere with
nodes corresponding to all the other physical registers. This is often called
{\em precoloring}. In fact, since we need to be careful about not accidentally
allocating physical register a different physical register, we need to make all physical
registers interfere with each other, this way they are all guaranteed to be
allocated their color (register). But these additional constrains can lead
to uncolorable (\"overconstrained") graphs if the live ranges of precolored
registers are too long. Since graph coloring maps each virtual register to a
single physical register, it needs the precolored live ranges to be short and
non-interfering, which can be done with {\em live range splitting} (see
section~\ref[sec:regalloc-splitting]).

But avoiding uncolorable graphs with splits means a lot of copy instructions,
which if allocated different registers, will not be optimized by peephole
optimization and thus can incur significant unnecessary overhead. This increases
the need for {\em coalescing} (see section~\ref[sec:regalloc-coalescing]).
Chaitin already realized the need for coalescing. His
solution~\cite[Chaitin1981] was to
coalesce every {\em copy-related}, {\em non-interfering} pair of virtual
registers in a pass called {\em coalesce}, before simplification. A pair of
virtual registers `t1` and `t2` is copy related when there is a copy (move)
instruction between them (i.e. `mov t1, t2` or vice versa), which captures the
goal of eliminating these moves. The virtual registers have to be
non-interfering, since otherwise the coalesced node would be uncolorable, and
also, since the temporaries interfere, they wouldn't be assigned distinct colors,
and elimination of the copy wouldn't be possible anyways. Because of the
non-intefering criterion, we have to be careful not to create artificial
interferences for the operands of a copy instruction, for example `mov t2, t1`
alone shouldn't imply that `t1` and `t2` interfere! Chaitin essentially does
coalescing everywhere where it is possible and where it {\em might} be
beneficial. Because of its nature, this form of coalescing has later become
called {\em aggressive}. The problem with it, is that the node created by
coalescing two virtual registers has interferences of both of the former nodes,
notably this means that the node's degree will be the sum of the two degrees and
such nodes easily become significant (\"high degree", not trivially colorable).
High degree nodes are problematic in the following {\em simplify} phase, because
apart from being blocked from simplification themselves, they prevent
simplifications on a high number of other nodes. Often this means spills of
these high degree nodes. Aggressive coloring can make colorable graphs
uncolorable, and depends on spilling to make the graph colorable again.
Since a spill of a node essentially splits the node into many low degree nodes,
this effectively undoes coalescing, but also adds memory operations that weren't
there originally.

Briggs improved on this by employing so called {\em conservative coalescing}.
Instead of coalescing all nodes that can be coalesced, he uses a filtering
heuristic, which allows only those coalesces, that can't make the graph
uncolorable. The filtering is done using a heuristic, because exactly predicting
the effect on colorability is a hard problem and would be too time consuming.
Since the effect of aggressive coalescing can be so severe, the heuristic was
made conservative, i.e. it never allows coalescings which would make the graph
uncolorable, but may also not allow coalesces that would be perfectly fine. The
heuristic says that `t1` and `t2` can be coalesced only when the merged node
`t12` would have no more significant (high-degree) neighbours, than $k$ (the
number of available registers). This implies, that after simplification of
(low-degree) neighbours, the node will have at most $k$ neighbours left, which
makes it simplifiable itself. Though it is easy to imagine a situation where a lot
of the neighbours have common neighbours, so simplification may do much better
than conservatively assumed, and thus a lot of the moves remain uncoalesced.

Appel and George found that for their use aggressive coalescing produced too
many spills, while conservative coalescing was too conservative, i.e. there were
are too many uneliminated move instructions left, even though coalescing would
be fine. They suggest an improvement called {\em iterated register coalescing}.
The idea is to still use only Briggs' conservative coalescing (to prevent making
the graph uncolorable), but instead of doing all the coalescing upfront, they
iterate the simplify and coalesce phases repeatedly. What is important is, that
simplify precedes coalescing---this alone improves the coalescing phase a lot,
since the conservative heuristic is based on degrees of nodes, and
simplification can decrease them significantly (and Briggs' coalescing heuristic
is too local to notice that otherwise). But importantly after coalescing
there may be nodes which become insignificant. For example, if `t3` interferes
with both `t1` and `t2`, and the two are coalesced into `t12`, then in effect
`t3` loses a neighbour and it's degree is decreased, and it might just become
insignificant (\"low degree") and simplifiable (leading to more simplifications,
which in turn might lead to more coalescings, etc.). While this may seem like a
perfect positive feedback loop, it is important to recall from TODO ref, that
coalescing two nodes creates a node of higher (even significant) degree.

Park and Moon note that even iterated coalescing can be too conservative and not
combine nodes that could be safely combined. They also note that that the
positive effect of coalescing explained in the previous paragraph is not to be
underestimated. Their approach is called {\em optimistic
coalescing}~\cite[Park2004], not to be confused with {\em optimistic coloring}
due to Briggs~\cite[Briggs1992].
Park and Moon's idea is to do aggressive coalescing like Chaitin did, to exploit the
positive effect of coalescing, but their improvement lies in being able to
revert coalescing of a particular node, if it would have to be spilled. Briggs'
optimistic coloring delayed actual spilling until the {\em assign} phase, since by
then it may turn out that the concrete assignment isn't as unfavorable as it
could have be just by judging from the interference graph and the simplification
heuristic. Similarly Park and Moon moves decisions to {\em not coalesce} into
the {\em assign} stage, and they are able to do better just because the concrete
assignment is known. For example, in case a color is not available for `t12`
(the result of coalescing `t1` and `t2`), it may be possible to find a color for
`t1` or `t2` (or both, though it will not be the same color), which effectively
undoes the coalescing. Though in practice, while Briggs' optimistic coloring
improvement was simple addition and a sure improvement, optimistic coalescing
and especially an efficient implementation is not simple.

Nice thing about Chaitin's scheme (and Briggs' improvement) is that even the
introduced spill code with new virtual registers gets the same general treatment
as other virtual registers - they are allocated by the next iteration of graph
coloring, so although the spill code needs to be inserted separately, it is
{\em not handled specially}. The price for this is that multiple expensive
iterations may be needed to finalize the allocation. An alternative would be to
(like with the top down allocator in section~\ref[sec:regalloc-top-down])
reserve a few registers off the side and use them to perform the loads and
stores around spilled variables. This could be used to rewrite the program into
final form after just one iteration of the graph coloring register allocator.
While this potentially saves multiple expensive iterations, it is less
flexible than coloring the spill code in a new iteration. In particular, since
the few spill handling registers have to be set off the side for the whole
program, we are not able to assign them, so in fact we are looking for a $k$
coloring for a smaller $k$ than the number of available registers, which
potentially means more spills by itself. On architectures like x86-64, where
some instructions only work with certain registers there is another difficulty
in choosing the on the side registers. If the registers needed by the
constrained instructions are put off the side, they would prevent any
allocation. But keeping them in the regular allocatable set would mean that
they won't be available for handling the spills of the values constrained to
such registers, the off the side registers would have to be used to somehow swap
the values with the needed registers.

\seccc Graph coloring of chordal graphs

While graph coloring in general is an NP-complete problem, for certain classes
of graphs, it can be easier. Notable example are chordal graphs, which have
posses useful properties for efficient graph coloring. It turns out, that
chordal graphs are the exact class of graphs for which exists a so called
\"perfect elimination order". Importantly for graph coloring, by assigning
colors to nodes in the perfect elimination order, the graph can
be colored with $k$ colors in a single greedy pass---of course provided that the
graph is indeed $k$-colorable. Similar greedy coloring pass was the {\em assign}
phase of Chaitin's algorithm~\cite[Chaitin1981], there the node ordering was
determined by simplifications based on a heuristic and colorability was ensured
by spilling nodes. Perfect elimination order can be found in $O(n^2)$ time using
the maximum cardinality search algorithm and {\em guarantees} optimal coloring.
Chordal graphs also offer improvements for spilling, because like with perfect
graphs of which they are a subset, the number of colors needed to color a
chordal graph is given by the size of the largest clique. This is powerful,
because it gives the possibility to do enough of spills or live range splits
ahead of time, before actually starting with coloring.

The first application of these ideas to register allocation are due to Pereira
and Palsberg~\cite[Pereira2005], who noticed that $95\,\%$ of
interference graphs in the Java 1.5 library had chordal interference graphs
(when compiled with JoeQ compiler). The algorithm proposed by them operates in a
few independent phases:

\begitems\style n
* {\em pre-spilling}. Spill code is inserted in order to decrease the size of
the largest clique to $k$, which makes it $k$ colorable.
* {\em greedy coloring}. The graph is greedily colored without limiting the
number of available colors.
* {\em post-spilling}. If the number of used colors exceeds $k$, additional
spills are done.
* {\em coalescing}. Move related nodes are coalesced, if possible.
\enditems

Both {\em pre-spilling} and {\em coalescing} are entirely optional---spills can
be handled by post-spilling phase and coalescing is not a necessary port of any
register allocation algorithm. But they both improve the quality of the
generated code. In particular due to the properties of the chordal graphs
described above, pre-spilling is a much better place for introducing spills, and
if done properly, it is guaranteed that post-spilling doesn't need to do
anything at all. Case when {\em post-spilling} comes into play are when the
optional pre-spilling isn't run, or when {\em non-chordal} graph is being
colored---Pereira and Palsberg noticed that the same algorithm can be used also
for non-chordal graphs, though the register assignment is not optimal, it is
competitive according to them~\cite[Pereira2005].

One of the important benefits of the algorithm is, that it isn't iterated, it
finishes after running each phase only once. Though to be more precise, the
post-spilling phase is not a single step---if more than $k$ are used, all nodes
of one color are spilled (transforming a graph using $m$ colors to one using
$m-1$), so this needs to be iterated until the only $k$ colors are used. Though
this is bounded, usually fast and may not be needed at all if {\em pre-spilling}
phase is run.

While there are interesting similarities to Chaitin's classical approach, in
particular being able to spill enough so that greedy algorithm can find a
coloring, there are also interesting differences---in Chaitin's and derived
algorithms coalescing is done {\em before} assignment, this can have both
positive and negative impacts on colorability, and different variations
approached it differently. Pereira's algorithm does all coalescing {\em after}
assignment because after coalescing an interference graph can become
non-chordal, though they report that in their experiments their approach
does better in coalescing than Appel's Iterated register coalescing.

While Pereira's algorithm can be used even for non-chordal interference graphs,
following researched by e.g. Hack~\cite[Hack2006] showed, that programs in
(strict) SSA form have chordal interference graphs. This is seems like an
excellent result, because SSA form is great for middle-end optimizations and it
simultaneously seems to be good for register allocation. Additionally SSA form
allows much more efficient computation of the liveness property. The basis
structure of Hack's algorithm is similar to Pereira's---do spilling before
coloring to make the program $k$ colorable, then color the graph in a perfect
elimination order. However, there are substantial improvements: as they prove
that interference is directly connected to the notion of dominance deeply
associated with SSA, it is possible to derive the perfect elimination order from
the control flow graph and dominator tree alone. Also, if we assume that copy
propagation has been run before register allocation, the only coalescing that
has to be done on SSA is the coalescing of $\phi$ node operands---by assigning
the operands the same color as the phi node, SSA deconstruction doesn't have to
insert any moves. However, in fact SSA deconstruction and coalescing have to be
integral parts of their algorithm, since inserting arbitrary moves for SSA
deconstruction could make the graph non-chordal (and in fact SSA incompatible)
or increase register demand---which would be detrimental, because now SSA
deconstruction is done {\em after} register allocation.

The fact that Hack's algorithm is able to take advantage of the many guarantees
of SSA form, is also it's great disadvantage---it depends on the program on
being in SSA form, which is not the norm. Usually, compilers do register
allocation only after instruction selection during which concrete machine
instructions are chosen to implement the behavior of the SSA-based intermediate
representation. $\phi$ instructions are not real instructions real
architectures, and certain operations are incompatible with SSA. TODO elaborate.

\seccc Reduction (to another NP-complete problem and using a solver)

ILP, IBQP

\chap Design and implementation

Přístupnost, jednoduchost =>

C (známé, žádné závisloti, možnost reprezentovat datové struktury přesně podle
potřeby)

malé množství IR (middleend + backend)

pokročilé a dobré algoritmy, ale ne úplně dnešní state of the art


\sec Architecture

Rozvržení fází, nějaké diagramy, přechody mezi reprezentacemi atd.

\sec Data structures

\secc Middle-end

SSA IR, inspirované LLVM

\secc Back-end

Machine independent representation of machine dependent instructions

x86-64 má 6 slotů, teoreticky je variabilní

\sec Lowering

Jednoduchá \"makro expanze" a SSA dekonstrukce (předpokládá absenci kritických
hran)

Problémy s 64 bit immediates, atd. Radši žádná case analysis, stejně ji musí
dělat peephole a ten je stejně povolanější, protože spojuje instrukce nehledě na
to z který middle end IR kód pochází

\sec Peephole optimization

\secc Local optimizations

\secc Flag based optimizations

\secc Use-def based optimizations

\secc Inter-block optimizations

\sec Register allocation

Iterated register coalescing

\chap Evaluation

Srovnání s GCC apod.

\chap Conclusion

\bibchap
\usebib/s (iso690) vlasami6-dip

\bye

This sequence simulates \"three address code" (see TODO) and the second instruction
is in fact what we started with to show use of spilled pseudoregisters.


As we have discussed in TODO, most instructions on
the x86-64 use the "two address code", where one of the operands is also the
destination for the result. Generally, three address code (where operations have
two operands and one destination that may or may not coincide with one of the
operands), are nicer from the perspective of the compiler---it is (at least in
principle, or while we are operating with pseudoregisters) non-destructive.





Coalescing vs live range splitting.



%The important takeaway is, that during register allocation by
%graph coloring, there are multiple potential causes for the graph becoming
%uncolorable, in any algorithm:
%
%\begitems
%* The graph needs more than $k$ colors to be colored.
%* The algorithm is too inexact to find a coloring.
%\enditems

needs to be noted that
more exact algorithm are necessarily able to find colorings even for graphs
which other algorithms would deem uncolorable. Briggs' approach is a 

The
interference graph has to change to 

\seccc Test

test




CHAITIN RECKLESS COALESCING

BRIGGS CONSERVATIVE COALESCING

GEORGE APPEL ITERATED REGISTER COALESCING



Since graph coloring can fail not only due to the graphs being uncolorable due
to interferences of precolored nodes, but also due to the graph having simply
needing more registers to be colored, we 

Since nodes corresponding to physical registers have a color associated with
them
Machine instruction constraints and calling conventions can be modelled bmachine constraints apply to {\em physical}
registers and by also adding them as nodes the interference graph, we can add
interferences between physical and virtual registers. This allows us to model
all usual constraints:

\begitems
* {\em Instructions allowing only specific input registers.} We 
\enditems

Graph coloring can fail not only because the graph's chromatic number is too
high, but also just because the particular graph coloring algorithm at hand is
{\em unable} to find a coloring.

so such
graphs (programs) need to be changed and graph coloring needs to be attempted
again. The changes






Zkratky:

DAG
JIT
SSA
